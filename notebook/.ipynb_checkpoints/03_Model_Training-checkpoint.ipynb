{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bed9ef99-5afc-4c30-b82c-4c915a81db31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciKeras is successfully linked to your project!\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE, SelectFromModel, mutual_info_classif\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(12, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "# Test if it initializes as a Scikit-Learn object\n",
    "clf = KerasClassifier(model=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "print(\"SciKeras is successfully linked to your project!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('heart_disease_eda_advanced.csv')\n",
    "# Drop temporary columns used in EDA\n",
    "drop_cols = ['iso_outlier', 'dbscan_outlier', 'ae_outlier', 'outlier_any', 'outlier_count',\n",
    "             'PC1', 'PC2', 'PCA1', 'PCA2', 'PCA3', 'tSNE1', 'tSNE2', 'UMAP1', 'UMAP2',\n",
    "             'kmeans_cluster', 'gmm_cluster', 'spectral_cluster']\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target'].astype(int)\n",
    "\n",
    "# Encode categorical variables (except target)\n",
    "categorical_cols = X.select_dtypes(include=['category']).columns.tolist()\n",
    "# Convert to numeric codes for modeling\n",
    "for col in categorical_cols:\n",
    "    X[col] = X[col].cat.codes\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87148ba8-9bdd-4602-a09a-00cf2853fc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by mutual information:\n",
      " cp                  0.156436\n",
      "thal                0.128619\n",
      "ca                  0.126923\n",
      "chol oldpeak        0.117666\n",
      "oldpeak             0.108542\n",
      "slope               0.098897\n",
      "thalach oldpeak     0.098522\n",
      "age oldpeak         0.096136\n",
      "exang               0.087338\n",
      "trestbps thalach    0.086173\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 2. Feature Selection \n",
    "\n",
    "\n",
    "\n",
    "# 1. Identify all non-numeric columns (Strings and Categories)\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 2. Convert them to numeric codes\n",
    "for col in categorical_cols:\n",
    "    # pd.factorize handles both strings and categories efficiently\n",
    "    X[col] = pd.factorize(X[col])[0]\n",
    "\n",
    "# 3. Handle any potential NaNs that might have been created or existed\n",
    "X = X.fillna(0) \n",
    "\n",
    "# Now you can re-run the split and Mutual Information\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 4. Feature Selection\n",
    "mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mutual information\n",
    "mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "mi_series = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "print(\"Top 10 features by mutual information:\\n\", mi_series.head(10))\n",
    "\n",
    "# Select top 10 features for modeling to reduce dimensionality\n",
    "top_features = mi_series.head(10).index.tolist()\n",
    "X_train_fs = X_train[top_features]\n",
    "X_test_fs = X_test[top_features] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed8bc54e-d55f-4c1c-9d47-21b76c8ce4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy   ROC-AUC        F1\n",
      "5             CatBoost  0.836066  0.896104  0.864865\n",
      "4             LightGBM  0.836066  0.892857  0.857143\n",
      "6    Gradient Boosting  0.852459  0.888528  0.873239\n",
      "1        Random Forest  0.786885  0.882576  0.821918\n",
      "3              XGBoost  0.836066  0.882035  0.861111\n",
      "2            SVM (RBF)  0.803279  0.871212  0.833333\n",
      "0  Logistic Regression  0.754098  0.844156  0.788732\n"
     ]
    }
   ],
   "source": [
    " # 3. Traditional Machine Learning Models\n",
    "\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]), random_state=42),\n",
    "    'LightGBM': LGBMClassifier(verbose=-1, class_weight='balanced', random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    # Scale for SVM and LR\n",
    "    if name in ['Logistic Regression', 'SVM (RBF)']:\n",
    "        pipeline = Pipeline([('scaler', StandardScaler()), ('model', model)])\n",
    "    else:\n",
    "        pipeline = Pipeline([('model', model)])\n",
    "    pipeline.fit(X_train_fs, y_train)\n",
    "    y_pred = pipeline.predict(X_test_fs)\n",
    "    y_proba = pipeline.predict_proba(X_test_fs)[:,1]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results.append({'Model': name, 'Accuracy': acc, 'ROC-AUC': auc, 'F1': f1})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('ROC-AUC', ascending=False)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "565f3d93-6f35-41f0-bd34-87ffaf2cf527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002A4A068E7A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000002A4A068E7A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Neural Network - Accuracy: 0.7704918032786885\n",
      "Neural Network - ROC-AUC: 0.8528138528138528\n"
     ]
    }
   ],
   "source": [
    "# 4. Deep Learning with TensorFlow/Keras \n",
    "\n",
    "# Build a function to create Keras model with tunable hyperparameters\n",
    "def create_keras_model(optimizer='adam', dropout_rate=0.2, neurons=64, n_layers=2, l2_reg=0.001):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(X_train_fs.shape[1],)))\n",
    "    for i in range(n_layers):\n",
    "        model.add(layers.Dense(neurons, activation='relu',\n",
    "                               kernel_regularizer=regularizers.l2(l2_reg)))\n",
    "        model.add(layers.Dropout(dropout_rate))\n",
    "        model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "# Wrap model for scikit-learn\n",
    "keras_clf = KerasClassifier(build_fn=create_keras_model, verbose=0)\n",
    "\n",
    "# Simple training (without tuning first)\n",
    "# Scale data for NN\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_scaled = scaler_nn.fit_transform(X_train_fs)\n",
    "X_test_scaled = scaler_nn.transform(X_test_fs)\n",
    "\n",
    "# Train a basic model\n",
    "model_nn = create_keras_model()\n",
    "history = model_nn.fit(X_train_scaled, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_nn = (model_nn.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
    "y_proba_nn = model_nn.predict(X_test_scaled).flatten()\n",
    "print(\"Neural Network - Accuracy:\", accuracy_score(y_test, y_pred_nn))\n",
    "print(\"Neural Network - ROC-AUC:\", roc_auc_score(y_test, y_proba_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d356e31f-d06e-43b6-a622-048f92613af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Wide & Deep - Accuracy: 0.7377049180327869\n",
      "Wide & Deep - ROC-AUC: 0.8679653679653679\n"
     ]
    }
   ],
   "source": [
    "# 5. Advanced Deep Learning: Wide & Deep Network (FIXED VERSION)\n",
    "\n",
    "# Wide & Deep: combines linear model with deep neural network\n",
    "def create_wide_deep_model(wide_features, deep_features, deep_units=[64,32], dropout=0.2):\n",
    "    # Wide part (linear)\n",
    "    # The shape is determined by the number of wide features passed in\n",
    "    wide_input = layers.Input(shape=(len(wide_features),), name='wide')\n",
    "    wide_out = layers.Dense(1, use_bias=False)(wide_input)  # linear combination\n",
    "    \n",
    "    # Deep part\n",
    "    deep_input = layers.Input(shape=(len(deep_features),), name='deep')\n",
    "    deep = layers.Dense(deep_units[0], activation='relu')(deep_input)\n",
    "    deep = layers.Dropout(dropout)(deep)\n",
    "    deep = layers.BatchNormalization()(deep)\n",
    "    for units in deep_units[1:]:\n",
    "        deep = layers.Dense(units, activation='relu')(deep)\n",
    "        deep = layers.Dropout(dropout)(deep)\n",
    "        deep = layers.BatchNormalization()(deep)\n",
    "    deep_out = layers.Dense(1, activation='linear')(deep)\n",
    "    \n",
    "    # Combine\n",
    "    combined = layers.Add()([wide_out, deep_out])\n",
    "    output = layers.Activation('sigmoid')(combined)\n",
    "    \n",
    "    model = keras.Model(inputs=[wide_input, deep_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    return model\n",
    "\n",
    "# --- FIX 1: Selection Logic ---\n",
    "# Define features\n",
    "wide_feats = ['age', 'trestbps', 'chol'] \n",
    "deep_feats = top_features # These are the columns currently in X_train_fs\n",
    "\n",
    "# IMPORTANT: We select wide features from X_train (full data) because \n",
    "# they might not have been selected in the top 10 'fs' (feature selection) step.\n",
    "X_train_wide = X_train[wide_feats].values\n",
    "X_test_wide = X_test[wide_feats].values\n",
    "\n",
    "# Deep features come from our already selected features\n",
    "X_train_deep = X_train_fs.values\n",
    "X_test_deep = X_test_fs.values\n",
    "\n",
    "# --- FIX 2: Scaling Logic ---\n",
    "# We must scale wide and deep inputs separately as they have different dimensions\n",
    "scaler_wide = StandardScaler().fit(X_train_wide)\n",
    "scaler_deep = StandardScaler().fit(X_train_deep)\n",
    "\n",
    "X_train_wide_scaled = scaler_wide.transform(X_train_wide)\n",
    "X_test_wide_scaled = scaler_wide.transform(X_test_wide)\n",
    "\n",
    "X_train_deep_scaled = scaler_deep.transform(X_train_deep)\n",
    "X_test_deep_scaled = scaler_deep.transform(X_test_deep)\n",
    "\n",
    "# --- Model Building & Training ---\n",
    "wide_deep_model = create_wide_deep_model(wide_feats, deep_feats)\n",
    "\n",
    "history_wd = wide_deep_model.fit(\n",
    "    [X_train_wide_scaled, X_train_deep_scaled], \n",
    "    y_train,\n",
    "    epochs=50, \n",
    "    batch_size=16, \n",
    "    validation_split=0.2, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# --- Prediction & Evaluation ---\n",
    "# predict returns a 2D array, we flatten it to 1D to match y_test\n",
    "y_proba_wd = wide_deep_model.predict([X_test_wide_scaled, X_test_deep_scaled]).flatten()\n",
    "y_pred_wd = (y_proba_wd > 0.5).astype(int)\n",
    "\n",
    "print(\"Wide & Deep - Accuracy:\", accuracy_score(y_test, y_pred_wd))\n",
    "print(\"Wide & Deep - ROC-AUC:\", roc_auc_score(y_test, y_proba_wd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c846fa4e-8e57-4789-987f-26f00b78820b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "NN with Class Weights - Accuracy: 0.7377049180327869\n",
      "NN with Class Weights - ROC-AUC: 0.8538961038961039\n"
     ]
    }
   ],
   "source": [
    "# 6. Handling Imbalance with Class Weights in TensorFlow \n",
    "\n",
    "# Compute class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "# Train same NN with class weights\n",
    "model_nn_weighted = create_keras_model()\n",
    "history_weighted = model_nn_weighted.fit(X_train_scaled, y_train, epochs=50, batch_size=16,\n",
    "                                         validation_split=0.2, class_weight=class_weight_dict, verbose=0)\n",
    "\n",
    "y_pred_nnw = (model_nn_weighted.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
    "y_proba_nnw = model_nn_weighted.predict(X_test_scaled).flatten()\n",
    "print(\"NN with Class Weights - Accuracy:\", accuracy_score(y_test, y_pred_nnw))\n",
    "print(\"NN with Class Weights - ROC-AUC:\", roc_auc_score(y_test, y_proba_nnw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05424e-ab7f-4be3-a32c-c614492e2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Autoencoder for Feature Extraction (Unsupervised Pre-training) \n",
    "\n",
    "# Train a sparse autoencoder to learn compressed representation\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 5\n",
    "\n",
    "input_layer = layers.Input(shape=(input_dim,))\n",
    "encoded = layers.Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "decoded = layers.Dense(input_dim, activation='linear')(encoded)\n",
    "autoencoder = keras.Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=100, batch_size=16, shuffle=True, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Encoder model to extract features\n",
    "encoder = keras.Model(input_layer, encoded)\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)\n",
    "\n",
    "# Train a classifier on encoded features\n",
    "clf_encoded = LogisticRegression(max_iter=1000)\n",
    "clf_encoded.fit(X_train_encoded, y_train)\n",
    "y_pred_enc = clf_encoded.predict(X_test_encoded)\n",
    "y_proba_enc = clf_encoded.predict_proba(X_test_encoded)[:,1]\n",
    "print(\"AE + Logistic Regression - Accuracy:\", accuracy_score(y_test, y_pred_enc))\n",
    "print(\"AE + Logistic Regression - ROC-AUC:\", roc_auc_score(y_test, y_proba_enc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad3cb1-536f-4742-a748-d272bd4c83ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2751124b-1dc2-4978-ad02-954853b91a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d95d4d-e318-4db6-ba9c-895969c15cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ad7421-abe4-410b-b8fe-6027dddd3046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7863340-773f-4a07-a78e-9d9af9a56daf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cc5045-8a33-42f3-abc8-5e4760e92ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c280fd6b-e90d-48cd-abdb-bd4fb5c48cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497954ff-f28a-4349-b655-392a775feffa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc74ea5-e875-4adb-a3f1-5dcf235afb41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e4078-f15f-480c-9b3b-d34703abedfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e103d-e6bd-403d-aaea-bd9f20b1a8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ab6c5-23f6-4255-b4c6-83904d853a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e5b7a-273c-4c08-9885-13afc406abdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34efa7f6-a71a-42ba-940d-d18db2056f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5132e-6f5a-41be-9b23-56ffdbe813f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7453dbf4-52fd-4125-911c-0d65f966c9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
