{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cec194-725e-4b10-b77b-1eee7597ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data ready: Used 10 top features.\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Scikit-learn modeling and validation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# --- FIX 1: Split calibration_curve from metrics ---\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             confusion_matrix, classification_report, roc_curve, precision_recall_curve,\n",
    "                             brier_score_loss)\n",
    "from sklearn.calibration import calibration_curve # Moved to its correct module\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, backend as K\n",
    "\n",
    "# --- FIX 2: Modern Keras Tuner naming ---\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "# Model Interpretability\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 2. Data Loading & Preprocessing\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('heart_disease_eda_advanced.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target'].astype(int)\n",
    "\n",
    "# --- FIX 3: Robust Categorical Encoding ---\n",
    "# This identifies both 'object' (strings like '60+') and 'category' types\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    # Using factorize handles strings better than .cat.codes if types aren't explicitly 'category'\n",
    "    X[col] = pd.factorize(X[col])[0]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 3. Feature Selection & Scaling\n",
    "\n",
    "# Reuse saved artifacts from previous steps\n",
    "try:\n",
    "    top_features = joblib.load('top_features.pkl')\n",
    "    scaler_nn = joblib.load('scaler_nn.pkl')\n",
    "    \n",
    "    # Filter to selected features\n",
    "    X_train_fs = X_train[top_features]\n",
    "    X_test_fs = X_test[top_features]\n",
    "\n",
    "    # Apply scaling\n",
    "    X_train_scaled = scaler_nn.transform(X_train_fs)\n",
    "    X_test_scaled = scaler_nn.transform(X_test_fs)\n",
    "    \n",
    "    print(\"Preprocessed data ready: Used\", len(top_features), \"top features.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: .pkl files not found. Ensure you saved 'top_features.pkl' and 'scaler_nn.pkl' in the previous notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c3501e-f940-4a9b-acc2-4321bd097705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 05s]\n",
      "val_auc: 0.9417563080787659\n",
      "\n",
      "Best val_auc So Far: 0.9507168531417847\n",
      "Total elapsed time: 00h 00m 52s\n",
      "\n",
      "==============================\n",
      "BEST HYPERPARAMETERS FOUND:\n",
      "Units: 128\n",
      "Layers: 1\n",
      "Dropout: 0.10\n",
      "Learning Rate: 0.0012\n",
      "L2 Regularization: 0.00013\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Hyperparameter Tuning for TensorFlow Models with Keras Tuner (FIXED)\n",
    "\n",
    "import keras_tuner as kt # Use the modern library name\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Clear any previous sessions to reset internal metric counters\n",
    "K.clear_session()\n",
    "\n",
    "# Define model building function for tuner\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    # Use the actual shape of your processed training data\n",
    "    model.add(layers.Input(shape=(X_train_fs.shape[1],)))\n",
    "    \n",
    "    # Hyperparameters to tune\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    hp_layers = hp.Int('layers', min_value=1, max_value=3)\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    hp_lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "    hp_l2 = hp.Float('l2', min_value=1e-5, max_value=1e-3, sampling='log')\n",
    "    \n",
    "    # Build hidden layers\n",
    "    for i in range(hp_layers):\n",
    "        model.add(layers.Dense(\n",
    "            units=hp_units, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=regularizers.l2(hp_l2)\n",
    "        ))\n",
    "        model.add(layers.Dropout(rate=hp_dropout))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        \n",
    "    # Output layer\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # FIX: Explicitly name the metric 'auc' so the tuner can find 'val_auc' reliably\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=hp_lr),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[keras.metrics.AUC(name='auc')] \n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Initialize the Tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    # FIX: Use the explicit Objective object\n",
    "    objective=kt.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1, # Set to 1 for faster debugging\n",
    "    directory='tuner_results',\n",
    "    project_name='heart_disease_v2', # Using a new name to avoid old log conflicts\n",
    "    overwrite=True # FIX: Automatically clear previous failed attempts/cache\n",
    ")\n",
    "\n",
    "# Start searching\n",
    "# Note: verbose=1 is better for debugging so you can see if an epoch fails\n",
    "tuner.search(\n",
    "    X_train_scaled, \n",
    "    y_train, \n",
    "    epochs=30, \n",
    "    validation_split=0.2, \n",
    "    batch_size=16, \n",
    "    verbose=1 \n",
    ")\n",
    "\n",
    "# --- Retrieve Results ---\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"BEST HYPERPARAMETERS FOUND:\")\n",
    "print(f\"Units: {best_hps.get('units')}\")\n",
    "print(f\"Layers: {best_hps.get('layers')}\")\n",
    "print(f\"Dropout: {best_hps.get('dropout'):.2f}\")\n",
    "print(f\"Learning Rate: {best_hps.get('learning_rate'):.4f}\")\n",
    "print(f\"L2 Regularization: {best_hps.get('l2'):.5f}\")\n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a86edba3-d341-433f-bf9d-27eba17d1c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Focal Loss Model - Accuracy: 0.6557377049180327\n",
      "Focal Loss Model - ROC-AUC: 0.6114718614718615\n"
     ]
    }
   ],
   "source": [
    "# 3. Cost-Sensitive Learning with Focal Loss\n",
    "\n",
    "\n",
    "def focal_loss(gamma=2., alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        cross_entropy = -y_true * K.log(y_pred)\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "        return K.mean(loss)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "# Build model with focal loss\n",
    "model_focal = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(X_train_fs.shape[1],)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_focal.compile(optimizer='adam', loss=focal_loss(alpha=0.25, gamma=2), metrics=['AUC'])\n",
    "\n",
    "history_focal = model_focal.fit(X_train_scaled, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n",
    "\n",
    "y_pred_focal = (model_focal.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
    "y_proba_focal = model_focal.predict(X_test_scaled).flatten()\n",
    "print(\"Focal Loss Model - Accuracy:\", accuracy_score(y_test, y_pred_focal))\n",
    "print(\"Focal Loss Model - ROC-AUC:\", roc_auc_score(y_test, y_proba_focal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7628b-dbc3-405d-b06c-b650de90aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Ensemble of Deep Learning Models (Averaging) \n",
    "\n",
    "# Train 3 different NN architectures\n",
    "models_ensemble = []\n",
    "for i in range(3):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.0005), input_shape=(X_train_fs.shape[1],)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "    model.fit(X_train_scaled, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=0)\n",
    "    models_ensemble.append(model)\n",
    "\n",
    "# Average predictions\n",
    "proba_ensemble = np.mean([m.predict(X_test_scaled).flatten() for m in models_ensemble], axis=0)\n",
    "y_pred_ensemble = (proba_ensemble > 0.5).astype(int)\n",
    "print(\"Ensemble of 3 NNs - Accuracy:\", accuracy_score(y_test, y_pred_ensemble))\n",
    "print(\"Ensemble of 3 NNs - ROC-AUC:\", roc_auc_score(y_test, proba_ensemble))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9741e90-2055-4111-8698-1898bb06b8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89e178-c2d6-44a1-9a01-62c1cb32d6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe0d05-ef93-4d1e-a8fb-346800e80d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91627bd8-2cbe-42ed-8266-011e3d446300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd22555-3d51-4f23-9bd1-a17bb99391bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf2f2e-d730-45a2-8185-0b08e1196a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c864b99-2708-4dda-ad9d-52a56df6b95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b92758-0cb8-4656-b416-3d2177657ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0d93a-cfff-4cbe-9431-00e3c2af8de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
