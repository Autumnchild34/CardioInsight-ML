{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cec194-725e-4b10-b77b-1eee7597ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data ready: Used 10 top features.\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "# Scikit-learn modeling and validation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# --- FIX 1: Split calibration_curve from metrics ---\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             confusion_matrix, classification_report, roc_curve, precision_recall_curve,\n",
    "                             brier_score_loss)\n",
    "from sklearn.calibration import calibration_curve # Moved to its correct module\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers, backend as K\n",
    "\n",
    "# --- FIX 2: Modern Keras Tuner naming ---\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "\n",
    "# Model Interpretability\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 2. Data Loading & Preprocessing\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('heart_disease_eda_advanced.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target'].astype(int)\n",
    "\n",
    "# --- FIX 3: Robust Categorical Encoding ---\n",
    "# This identifies both 'object' (strings like '60+') and 'category' types\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    # Using factorize handles strings better than .cat.codes if types aren't explicitly 'category'\n",
    "    X[col] = pd.factorize(X[col])[0]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 3. Feature Selection & Scaling\n",
    "\n",
    "# Reuse saved artifacts from previous steps\n",
    "try:\n",
    "    top_features = joblib.load('top_features.pkl')\n",
    "    scaler_nn = joblib.load('scaler_nn.pkl')\n",
    "    \n",
    "    # Filter to selected features\n",
    "    X_train_fs = X_train[top_features]\n",
    "    X_test_fs = X_test[top_features]\n",
    "\n",
    "    # Apply scaling\n",
    "    X_train_scaled = scaler_nn.transform(X_train_fs)\n",
    "    X_test_scaled = scaler_nn.transform(X_test_fs)\n",
    "    \n",
    "    print(\"Preprocessed data ready: Used\", len(top_features), \"top features.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: .pkl files not found. Ensure you saved 'top_features.pkl' and 'scaler_nn.pkl' in the previous notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c3501e-f940-4a9b-acc2-4321bd097705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86edba3-d341-433f-bf9d-27eba17d1c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7628b-dbc3-405d-b06c-b650de90aaaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9741e90-2055-4111-8698-1898bb06b8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89e178-c2d6-44a1-9a01-62c1cb32d6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe0d05-ef93-4d1e-a8fb-346800e80d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91627bd8-2cbe-42ed-8266-011e3d446300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd22555-3d51-4f23-9bd1-a17bb99391bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf2f2e-d730-45a2-8185-0b08e1196a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c864b99-2708-4dda-ad9d-52a56df6b95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b92758-0cb8-4656-b416-3d2177657ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf0d93a-cfff-4cbe-9431-00e3c2af8de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
